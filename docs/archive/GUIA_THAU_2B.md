# GuÃ­a Completa THAU-2B
## Modelo Propio con Auto-Aprendizaje y Crecimiento Progresivo

---

## Resumen Ejecutivo

THAU-2B es un modelo de lenguaje de **2B parÃ¡metros** entrenado **desde cero** con capacidades Ãºnicas:

- **ğŸŒ± Crecimiento Progresivo**: El modelo empieza pequeÃ±o y crece gradualmente
- **ğŸ§  Auto-Aprendizaje**: Genera sus propias preguntas y aprende de ellas
- **ğŸ’­ Auto-Preguntas**: Sistema autÃ³nomo de generaciÃ³n de conocimiento
- **ğŸ“š Auto-GeneraciÃ³n de Datasets**: Crea sus propios datos de entrenamiento
- **ğŸ¯ Aprendizaje Incremental**: Mejora continuamente sin reentrenamiento completo

---

## Arquitectura THAU-2B

### ConfiguraciÃ³n del Modelo

```python
THAU_2B_CONFIG = TransformerConfig(
    vocab_size=32000,
    d_model=2560,          # DimensiÃ³n oculta
    n_heads=32,            # Cabezas de atenciÃ³n
    n_layers=24,           # Capas transformer
    d_ff=10240,            # DimensiÃ³n feed-forward (4x d_model)
    max_seq_length=4096,   # Ventana de contexto
    use_rotary_embeddings=True,  # RoPE para mejor posicionamiento
)
```

### ParÃ¡metros Totales

- **Embeddings**: 32,000 Ã— 2,560 = ~82M
- **Por capa**: ~78.6M parÃ¡metros
- **24 capas**: 24 Ã— 78.6M = ~1.9B
- **Total**: **~2B parÃ¡metros**

---

## Sistema de Crecimiento Progresivo

THAU crece por "edades cognitivas", como un ser humano:

| Edad | ParÃ¡metros | d_model | Capas | Capacidades |
|------|-----------|---------|-------|-------------|
| 0 | 18M | 256 | 2 | Palabras bÃ¡sicas |
| 1 | 35M | 384 | 3 | Frases simples |
| 3 | 110M | 512 | 6 | Explicaciones bÃ¡sicas |
| 6 | 450M | 768 | 12 | Razonamiento bÃ¡sico |
| 12 | 1B | 1024 | 24 | Razonamiento complejo |
| **15** | **2B** | **2560** | **24** | **THAU-2B completo** |

---

## Sistema de Auto-Aprendizaje

### 1. Auto-Preguntas (Self-Questioning)

El sistema genera preguntas automÃ¡ticamente segÃºn la edad cognitiva:

```python
# Edad 0: Preguntas bÃ¡sicas
"Â¿QuÃ© es {concepto}?"
"Â¿CÃ³mo se usa {concepto}?"

# Edad 3+: Preguntas complejas
"Â¿CÃ³mo se relaciona {concepto1} con {concepto2}?"
"Â¿CuÃ¡les son las ventajas de {concepto}?"
```

**LÃ­mites de Seguridad:**
- MÃ¡ximo 10 preguntas/hora
- MÃ¡ximo 100 preguntas/dÃ­a
- 30 segundos mÃ­nimo entre preguntas

### 2. DetecciÃ³n de Brechas de Conocimiento

El sistema detecta automÃ¡ticamente cuando no sabe algo:

- Respuestas muy cortas (< 20 caracteres)
- Marcadores de incertidumbre ("no estoy seguro", "no sÃ©")
- Confianza baja (< 0.6)

### 3. Auto-GeneraciÃ³n de Datasets

Cuando detecta brechas, genera automÃ¡ticamente datasets para cubr irlas:

```python
# Ejemplo: Brecha detectada en "algoritmos"
â†’ Genera 5-10 ejemplos sobre algoritmos
â†’ Guarda en data/datasets/auto_generated/
â†’ Entrena con los nuevos ejemplos
```

---

## CÃ³mo Entrenar THAU-2B

### OpciÃ³n 1: Entrenamiento Progresivo Completo

Entrena desde edad 0 hasta edad 15 (THAU-2B):

```bash
# Activar entorno virtual
source venv/bin/activate

# Entrenar hasta edad 15 (THAU-2B)
python train_thau_2b.py --target-age 15
```

**Tiempo estimado**: 5-10 horas (dependiendo del hardware)

### OpciÃ³n 2: Entrenamiento por Fases

Entrena gradualmente edad por edad:

```bash
# Fase 1: Edad 0 (bebÃ©)
python train_thau_2b.py --target-age 0

# Fase 2: Edad 1 (niÃ±o)
python train_thau_2b.py --target-age 1

# Fase 3: Edad 3 (escolar)
python train_thau_2b.py --target-age 3

# ...hasta edad 15
python train_thau_2b.py --target-age 15
```

### OpciÃ³n 3: Continuar Entrenamiento Existente

Si ya tienes un checkpoint:

```python
from thau_trainer.own_model_manager import ThauOwnModelManager

manager = ThauOwnModelManager()
manager.load_checkpoint(Path("./data/model_checkpoints/age_12_final.pt"))
manager.advance_age(15)  # Avanzar a THAU-2B
```

---

## Ciclo de Auto-Aprendizaje

El entrenamiento sigue este ciclo automÃ¡tico:

```
1. [Bootstrap] â†’ Datos iniciales bÃ¡sicos
         â†“
2. [Self-Question] â†’ Genera pregunta automÃ¡tica
         â†“
3. [Answer] â†’ Responde usando el modelo
         â†“
4. [Detect Gap] â†’ Â¿Respuesta de baja calidad?
         â†“
5. [Generate Dataset] â†’ Crea ejemplos para mejorar
         â†“
6. [Train] â†’ Entrena con nuevos ejemplos
         â†“
7. [Save Checkpoint] â†’ Guarda progreso
         â†“
8. [Repeat] â†’ Vuelve al paso 2
```

---

## Estructura de Archivos

```
my-llm/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ model_configs.py          # THAU_2B_CONFIG agregado
â”œâ”€â”€ thau_trainer/
â”‚   â”œâ”€â”€ own_model_manager.py      # Gestor del modelo propio (actualizado)
â”‚   â”œâ”€â”€ self_questioning.py       # Sistema de auto-preguntas
â”‚   â””â”€â”€ self_learning.py          # Auto-generaciÃ³n de datasets
â”œâ”€â”€ train_thau_2b.py              # Script principal de entrenamiento
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ model_checkpoints/        # Checkpoints guardados
â”‚   â”œâ”€â”€ datasets/auto_generated/  # Datasets auto-generados
â”‚   â”œâ”€â”€ training_stats/           # EstadÃ­sticas por fase
â”‚   â”œâ”€â”€ logs/                     # Logs del sistema
â”‚   â””â”€â”€ self_questioning/         # Preguntas generadas
â””â”€â”€ export/
    â”œâ”€â”€ export_to_gguf.py         # Exportador a formato GGUF
    â””â”€â”€ Modelfile-thau            # ConfiguraciÃ³n para Ollama
```

---

## Exportar a Ollama

Una vez entrenado THAU-2B, exporta a Ollama:

### 1. Exportar a GGUF

```bash
python export/export_to_gguf.py \
  --model-path ./data/model_checkpoints/age_15_final.pt \
  --output-dir ./export/models \
  --quantization Q4_K_M
```

### 2. Crear Modelo en Ollama

```bash
# Crear Modelfile personalizado
cat > export/Modelfile-thau-2b <<EOF
FROM ./export/models/thau-2b-Q4_K_M.gguf

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER num_ctx 4096

SYSTEM """Eres THAU, un modelo de 2B parÃ¡metros entrenado desde cero con
capacidades de auto-aprendizaje y razonamiento avanzado..."""
EOF

# Importar a Ollama
ollama create thau-2b -f export/Modelfile-thau-2b
```

### 3. Usar THAU-2B

```bash
# Modo interactivo
ollama run thau-2b

# Consulta directa
ollama run thau-2b "Explica quÃ© es Clean Architecture"

# Desde Python
import ollama
response = ollama.chat(model='thau-2b', messages=[
    {'role': 'user', 'content': 'Â¿QuÃ© es SOLID?'}
])
print(response['message']['content'])
```

---

## Monitoreo y MÃ©tricas

### Ver EstadÃ­sticas de Entrenamiento

```python
from thau_trainer.own_model_manager import ThauOwnModelManager
import json

manager = ThauOwnModelManager()
manager.load_checkpoint(Path("./data/model_checkpoints/age_15_final.pt"))

stats = manager.get_stats()
print(json.dumps(stats, indent=2))
```

### Ver EstadÃ­sticas de Auto-Aprendizaje

```python
from thau_trainer.self_learning import SelfLearningManager

self_learning = SelfLearningManager()
stats = self_learning.get_stats()

print(f"Brechas detectadas: {stats['total_gaps_detected']}")
print(f"Datasets generados: {stats['total_datasets_generated']}")
print(f"Ejemplos totales: {stats['total_examples_generated']}")
```

### Ver Logs de Preguntas Auto-Generadas

```bash
# Ver Ãºltimas 10 preguntas
tail -10 data/self_questioning/activity_log.json

# Ver todas las Q&A del dÃ­a
cat data/self_questioning/qa_$(date +%Y%m%d).jsonl
```

---

## Mejores PrÃ¡cticas

### 1. Entrenamiento Incremental

- âœ… Entrena gradualmente por edades
- âœ… Guarda checkpoints cada 25 steps
- âœ… Monitorea la perplexity (debe bajar)
- âŒ No saltes edades abruptamente

### 2. Auto-Aprendizaje

- âœ… Respeta los lÃ­mites de preguntas/hora
- âœ… Revisa las brechas detectadas periÃ³dicamente
- âœ… Valida los datasets auto-generados
- âŒ No deshabilites los lÃ­mites de seguridad

### 3. Recursos

- **RAM**: MÃ­nimo 16GB (32GB recomendado)
- **GPU**: MPS (Apple Silicon) o CUDA
- **Disco**: 20GB libres para checkpoints
- **Tiempo**: 5-10 horas para entrenamiento completo

---

## SoluciÃ³n de Problemas

### Error: "CUDA out of memory" / "MPS out of memory"

```python
# Reducir batch size en train_thau_2b.py
gradient_accumulation_steps=8  # Aumentar de 2 a 8
```

### Error: "Tokenizer retorna string"

Ya corregido en `own_model_manager.py` lÃ­neas 211-218 y 316-321

### El entrenamiento es muy lento

```bash
# Reducir steps por fase
python train_thau_2b.py --target-age 3 --steps-per-age 25
```

### No genera auto-preguntas

Verifica que Ollama estÃ© corriendo:

```bash
ollama list  # Debe mostrar modelos disponibles
```

---

## PrÃ³ximos Pasos

1. **Entrenar THAU-2B**: Ejecuta `python train_thau_2b.py --target-age 15`
2. **Exportar a Ollama**: Usa el modelo entrenado en producciÃ³n
3. **Continuar Aprendizaje**: Deja que THAU se auto-entrene continuamente
4. **Fine-tuning Especializado**: Entrena en dominios especÃ­ficos

---

## Comandos RÃ¡pidos

```bash
# Ver modelos disponibles en Ollama
ollama list

# Usar THAU actual (TinyLlama 1.1B)
ollama run thau:latest

# Entrenar THAU-2B desde cero
python train_thau_2b.py --target-age 15

# Verificar progreso
tail -f data/training_output.log

# Ver checkpoints guardados
ls -lh data/model_checkpoints/

# Exportar modelo entrenado
python export/export_to_gguf.py
```

---

## Contacto y Soporte

- **Logs**: `data/logs/`
- **Checkpoints**: `data/model_checkpoints/`
- **Datasets**: `data/datasets/auto_generated/`
- **EstadÃ­sticas**: `data/training_stats/`

---

**Â¡THAU-2B estÃ¡ listo para crecer y aprender! ğŸš€**
