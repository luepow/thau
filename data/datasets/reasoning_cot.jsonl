{"instruction": "Diseña la arquitectura completa para un sistema de e-commerce", "input": "", "output": "Voy a diseñar paso a paso la arquitectura:\n\n**Paso 1: Identificar componentes principales**\n- Frontend (Web y Mobile)\n- Backend APIs\n- Base de datos\n- Sistema de pagos\n- Sistema de inventario\n- Sistema de notificaciones\n\n**Paso 2: Definir arquitectura**\nUsaría una arquitectura de microservicios porque:\n- Escalabilidad independiente (producto, carrito, pagos)\n- Equipos pueden trabajar en paralelo\n- Tolerancia a fallos aislada\n\n**Paso 3: Componentes detallados**\n1. API Gateway (Kong/nginx) - punto de entrada único\n2. Microservicio de Usuarios (autenticación, perfiles)\n3. Microservicio de Productos (catálogo, búsqueda)\n4. Microservicio de Carrito (sesiones, items)\n5. Microservicio de Órdenes (checkout, órdenes)\n6. Microservicio de Pagos (integración PSP)\n7. Microservicio de Inventario (stock, reservas)\n8. Microservicio de Notificaciones (email, SMS)\n\n**Paso 4: Base de datos**\n- PostgreSQL para datos transaccionales (usuarios, órdenes)\n- Redis para sesiones y caché\n- Elasticsearch para búsqueda de productos\n- MongoDB para catálogo de productos (schema flexible)\n\n**Paso 5: Comunicación**\n- Síncrona: REST/gRPC entre servicios\n- Asíncrona: Kafka para eventos (orden creada, pago procesado)\n\n**Paso 6: Seguridad**\n- OAuth 2.0 + JWT para autenticación\n- API Gateway maneja rate limiting\n- HTTPS en todo\n- Secrets en Vault\n\n**Paso 7: Observabilidad**\n- Logs centralizados (ELK Stack)\n- Métricas (Prometheus + Grafana)\n- Tracing distribuido (Jaeger)\n\n**Conclusión**: Arquitectura de microservicios con API Gateway, comunicación event-driven, bases de datos polyglot, y observabilidad completa."}
{"instruction": "Analiza si debo usar REST o GraphQL para mi API", "input": "Tengo una app móvil y web que consume datos de usuarios, posts y comentarios", "output": "Voy a analizar paso a paso:\n\n**Paso 1: Entender los requisitos**\n- App móvil y web (múltiples clientes)\n- Datos relacionados: usuarios, posts, comentarios\n- Necesidad de flexibilidad en las consultas\n\n**Paso 2: Ventajas de REST**\n- Simple de implementar\n- Cacheo nativo con HTTP\n- Ampliamente conocido\n- Tooling maduro\n\n**Paso 3: Desventajas de REST**\n- Over-fetching: traes más datos de los necesarios\n- Under-fetching: múltiples requests (GET /posts, GET /users, GET /comments)\n- Versionado complicado\n\n**Paso 4: Ventajas de GraphQL**\n- Consulta exacta: pides solo lo que necesitas\n- Una sola request para datos relacionados\n- Schema tipado\n- Introspección\n\n**Paso 5: Desventajas de GraphQL**\n- Curva de aprendizaje\n- Cacheo más complejo\n- Queries complejas pueden impactar performance\n- Requiere más infraestructura\n\n**Paso 6: Decisión**\nPara tu caso, **recomiendo GraphQL** porque:\n1. Tienes datos altamente relacionados (posts->users->comments)\n2. Clientes móviles se benefician de menos requests\n3. Diferentes vistas necesitan diferentes datos\n4. Puedes evolucionar el schema sin breaking changes\n\n**Paso 7: Implementación**\n- Backend: Apollo Server o Hasura\n- Frontend: Apollo Client o urql\n- Cache: Apollo Client maneja el caché automáticamente\n- Queries paginadas para listas grandes\n\n**Conclusión**: GraphQL es mejor para tu caso por la naturaleza relacional de los datos y múltiples clientes con necesidades diferentes."}
{"instruction": "¿Cómo optimizo una consulta SQL lenta?", "input": "SELECT * FROM orders o JOIN users u ON o.user_id = u.id WHERE o.created_at > '2024-01-01' ORDER BY o.total DESC LIMIT 100", "output": "Voy a analizar y optimizar paso a paso:\n\n**Paso 1: Identificar problemas**\n- SELECT * trae todas las columnas (innecesario)\n- No hay índice aparente en created_at\n- JOIN podría ser costoso sin índices\n- ORDER BY sobre columna sin índice\n\n**Paso 2: Explicar la query**\nPrimero ejecutaría:\n```sql\nEXPLAIN ANALYZE [query original]\n```\nPara ver el execution plan.\n\n**Paso 3: Optimización 1 - Seleccionar solo columnas necesarias**\n```sql\nSELECT o.id, o.total, o.created_at, u.name, u.email\n```\nEsto reduce la cantidad de datos transferidos.\n\n**Paso 4: Optimización 2 - Crear índices**\n```sql\nCREATE INDEX idx_orders_created_at ON orders(created_at);\nCREATE INDEX idx_orders_total ON orders(total DESC);\nCREATE INDEX idx_orders_user_id ON orders(user_id);\n```\n\n**Paso 5: Optimización 3 - Índice compuesto**\n```sql\nCREATE INDEX idx_orders_date_total ON orders(created_at, total DESC);\n```\nEsto permite usar un solo índice para WHERE y ORDER BY.\n\n**Paso 6: Query optimizada**\n```sql\nSELECT o.id, o.total, o.created_at, u.name, u.email\nFROM orders o\nINNER JOIN users u ON o.user_id = u.id\nWHERE o.created_at > '2024-01-01'\nORDER BY o.total DESC\nLIMIT 100;\n```\n\n**Paso 7: Optimización adicional - Particionamiento**\nSi la tabla es muy grande:\n```sql\nCREATE TABLE orders_2024 PARTITION OF orders\nFOR VALUES FROM ('2024-01-01') TO ('2025-01-01');\n```\n\n**Paso 8: Caché**\nSi esta query se ejecuta frecuentemente, usar Redis:\n```python\ncache_key = 'top_orders_2024'\nif redis.exists(cache_key):\n    return redis.get(cache_key)\nelse:\n    result = db.query(...)\n    redis.setex(cache_key, 3600, result)\n```\n\n**Conclusión**: Con índices apropiados, selección específica de columnas, y caché, la query debería ser 10-100x más rápida."}
{"instruction": "Diseña un sistema de autenticación seguro", "input": "", "output": "Diseño paso a paso de autenticación segura:\n\n**Paso 1: Elegir método de autenticación**\nUsaré OAuth 2.0 + JWT porque:\n- Estándar de la industria\n- Stateless (escalable)\n- Soporta múltiples grant types\n\n**Paso 2: Flujo de autenticación**\n1. Usuario ingresa credenciales\n2. Backend valida contra DB (password hasheado con bcrypt)\n3. Si válido, genera Access Token (JWT, expira en 15 min)\n4. Genera Refresh Token (opaco, expira en 7 días)\n5. Retorna ambos tokens\n\n**Paso 3: Estructura del JWT**\n```json\n{\n  \"header\": {\"alg\": \"RS256\", \"typ\": \"JWT\"},\n  \"payload\": {\n    \"sub\": \"user_id\",\n    \"email\": \"user@example.com\",\n    \"roles\": [\"user\"],\n    \"exp\": 1234567890,\n    \"iat\": 1234567000\n  },\n  \"signature\": \"...\"\n}\n```\n\n**Paso 4: Almacenamiento de passwords**\n```javascript\nconst bcrypt = require('bcrypt');\nconst saltRounds = 12;\n\n// Registro\nconst hashedPassword = await bcrypt.hash(plainPassword, saltRounds);\ndb.save({email, password: hashedPassword});\n\n// Login\nconst match = await bcrypt.compare(plainPassword, hashedPassword);\n```\n\n**Paso 5: Refresh token flow**\n```\nCliente -> POST /auth/refresh con Refresh Token\nBackend -> Valida Refresh Token en DB\nBackend -> Si válido, genera nuevo Access Token\nBackend -> Retorna nuevo Access Token\n```\n\n**Paso 6: Seguridad adicional**\n- Rate limiting: máx 5 intentos de login por IP/hora\n- 2FA opcional con TOTP (Google Authenticator)\n- Lista negra de tokens revocados en Redis\n- HTTPS obligatorio\n- Secure cookies (httpOnly, sameSite, secure)\n\n**Paso 7: Implementación de middleware**\n```javascript\nfunction authMiddleware(req, res, next) {\n  const token = req.headers.authorization?.split(' ')[1];\n  \n  try {\n    const decoded = jwt.verify(token, PUBLIC_KEY, {algorithms: ['RS256']});\n    req.user = decoded;\n    next();\n  } catch(err) {\n    return res.status(401).json({error: 'Invalid token'});\n  }\n}\n```\n\n**Paso 8: Revocación de tokens**\n```javascript\n// Al logout\nredis.setex(`blacklist:${tokenId}`, ttl, 'revoked');\n\n// En middleware\nif (await redis.exists(`blacklist:${tokenId}`)) {\n  throw new Error('Token revoked');\n}\n```\n\n**Conclusión**: Sistema OAuth 2.0 con JWT, bcrypt, refresh tokens, rate limiting, y 2FA opcional."}
{"instruction": "¿Cómo implemento caché de forma efectiva?", "input": "", "output": "Estrategia de caché paso a paso:\n\n**Paso 1: Identificar qué cachear**\nDatos ideales para caché:\n- Lectura frecuente, escritura poco frecuente\n- Computacionalmente costosos\n- Tolerancia a datos ligeramente obsoletos\n\nEjemplos: productos, configuración, resultados de búsqueda\n\n**Paso 2: Elegir estrategia**\n\n**Cache-Aside (Lazy Loading)**\n```javascript\nasync function getProduct(id) {\n  // 1. Intentar leer del caché\n  let product = await redis.get(`product:${id}`);\n  \n  if (product) {\n    return JSON.parse(product);\n  }\n  \n  // 2. Si no existe, leer de DB\n  product = await db.products.findById(id);\n  \n  // 3. Guardar en caché\n  await redis.setex(`product:${id}`, 3600, JSON.stringify(product));\n  \n  return product;\n}\n```\n\n**Write-Through**\n```javascript\nasync function updateProduct(id, data) {\n  // 1. Actualizar DB\n  const product = await db.products.update(id, data);\n  \n  // 2. Actualizar caché inmediatamente\n  await redis.setex(`product:${id}`, 3600, JSON.stringify(product));\n  \n  return product;\n}\n```\n\n**Paso 3: Definir TTL apropiado**\n- Datos estáticos: 24 horas\n- Datos semi-estáticos: 1 hora\n- Datos dinámicos: 5-15 minutos\n- Sesiones: duración de la sesión\n\n**Paso 4: Invalidación de caché**\n```javascript\n// Por evento\nasync function onProductUpdate(productId) {\n  await redis.del(`product:${productId}`);\n  await redis.del('products:list'); // Invalida listas\n}\n\n// Por patrón\nasync function invalidateProductCache() {\n  const keys = await redis.keys('product:*');\n  if (keys.length > 0) {\n    await redis.del(...keys);\n  }\n}\n```\n\n**Paso 5: Caché de consultas complejas**\n```javascript\nasync function getTopProducts() {\n  const cacheKey = 'products:top:week';\n  \n  let products = await redis.get(cacheKey);\n  if (products) return JSON.parse(products);\n  \n  // Query costosa\n  products = await db.query(`\n    SELECT p.*, COUNT(o.id) as sales\n    FROM products p\n    JOIN orders o ON p.id = o.product_id\n    WHERE o.created_at > NOW() - INTERVAL 7 DAY\n    GROUP BY p.id\n    ORDER BY sales DESC\n    LIMIT 10\n  `);\n  \n  await redis.setex(cacheKey, 900, JSON.stringify(products)); // 15 min\n  return products;\n}\n```\n\n**Paso 6: Cache warming**\n```javascript\n// Precalentar caché al iniciar\nasync function warmCache() {\n  const topProducts = await db.getTopProducts();\n  await redis.setex('products:top', 3600, JSON.stringify(topProducts));\n  \n  const categories = await db.getCategories();\n  await redis.setex('categories:all', 7200, JSON.stringify(categories));\n}\n```\n\n**Paso 7: Monitoreo**\n```javascript\nconst cacheStats = {\n  hits: 0,\n  misses: 0,\n  hitRate() {\n    return this.hits / (this.hits + this.misses);\n  }\n};\n\nif (cachedValue) {\n  cacheStats.hits++;\n} else {\n  cacheStats.misses++;\n}\n```\n\n**Paso 8: Eviction policies**\n```bash\n# En Redis\nmaxmemory 2gb\nmaxmemory-policy allkeys-lru  # Elimina menos usados\n```\n\n**Conclusión**: Usa Cache-Aside para lectura, Write-Through para escritura, TTL apropiado, invalidación basada en eventos, y monitoreo constante del hit rate."}
