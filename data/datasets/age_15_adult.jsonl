{"instruction": "Diseña una arquitectura de microservicios para un sistema de e-commerce escalable", "input": "", "output": "Voy a diseñar una arquitectura de microservicios completa para e-commerce con los siguientes componentes:\n\n## Arquitectura de Alto Nivel\n\n### 1. Capa de Cliente\n- **Web App**: React/Next.js con SSR para SEO\n- **Mobile App**: React Native o Flutter\n- **Admin Dashboard**: Vue.js o React\n\n### 2. API Gateway (Kong o AWS API Gateway)\n**Responsabilidades:**\n- Routing inteligente a microservicios\n- Rate limiting (1000 req/min por usuario)\n- Autenticación JWT\n- Caching de respuestas frecuentes\n- Logging centralizado\n\n### 3. Microservicios Core\n\n**A. User Service**\n- Tech: Node.js + Express + PostgreSQL\n- Endpoints: registro, login, perfil, preferencias\n- Implementa OAuth2 + JWT\n- Base de datos: PostgreSQL (ACID para datos críticos)\n- Cache: Redis para sesiones\n\n**B. Product Service**\n- Tech: Python + FastAPI + MongoDB\n- Endpoints: catálogo, búsqueda, filtros, recomendaciones\n- Base de datos: MongoDB (flexibilidad para atributos variados)\n- Search: Elasticsearch para búsqueda full-text\n- CDN: Cloudflare/CloudFront para imágenes\n\n**C. Inventory Service**\n- Tech: Java + Spring Boot + PostgreSQL\n- Endpoints: stock, reservas, actualizaciones\n- Patrón SAGA para transacciones distribuidas\n- Event sourcing para trazabilidad completa\n- Cache: Redis con invalidación por eventos\n\n**D. Order Service**\n- Tech: Go + Gin + PostgreSQL\n- Endpoints: creación, actualización, historial\n- State machine para estados de orden\n- Transacciones con compensación (2PC no recomendado)\n- Garantiza idempotencia con request IDs\n\n**E. Payment Service**\n- Tech: Java + Spring Boot + PostgreSQL\n- Integraciones: Stripe, PayPal, etc.\n- PCI DSS compliance\n- Tokenización de tarjetas (nunca almacenar datos sensibles)\n- Webhook handlers para confirmaciones asíncronas\n- Retry logic con exponential backoff\n\n**F. Shipping Service**\n- Tech: Python + Flask + PostgreSQL\n- Integraciones: FedEx, UPS, DHL APIs\n- Cálculo de costos en tiempo real\n- Tracking de paquetes\n- Optimización de rutas\n\n**G. Notification Service**\n- Tech: Node.js + Bull (queue) + Redis\n- Canales: Email (SendGrid), SMS (Twilio), Push (FCM)\n- Templates con Handlebars\n- Retry queue para fallos\n- Preferencias de usuario respetadas\n\n**H. Analytics Service**\n- Tech: Python + Spark + Kafka\n- Event streaming con Kafka\n- Procesamiento batch y tiempo real\n- Data warehouse: Snowflake o BigQuery\n- BI: Metabase o Tableau\n\n### 4. Comunicación entre Servicios\n\n**Síncrona:**\n- gRPC para comunicación interna (más rápido que REST)\n- REST para APIs públicas\n- Circuit breaker: Hystrix o Resilience4j\n- Timeout: 3s default, 10s máximo\n\n**Asíncrona:**\n- Message broker: Kafka para eventos de negocio\n- RabbitMQ para tareas asíncronas\n- Patrón Event-Driven Architecture\n\n### 5. Bases de Datos\n\n**Polyglot Persistence:**\n- PostgreSQL: Orders, Users, Payments (ACID requerido)\n- MongoDB: Products (esquema flexible)\n- Redis: Cache, sesiones, rate limiting\n- Elasticsearch: Búsqueda de productos\n- Cassandra: Logs, Analytics (write-heavy)\n\n**Estrategia de backups:**\n- Snapshots automáticos cada 6h\n- WAL archiving continuo\n- Replicación multi-región\n\n### 6. Observabilidad\n\n**Logging:**\n- Stack ELK (Elasticsearch, Logstash, Kibana)\n- Formato JSON estructurado\n- Trace ID propagado en todos los servicios\n\n**Monitoring:**\n- Prometheus para métricas\n- Grafana para dashboards\n- Alertas: PagerDuty o OpsGenie\n\n**Tracing:**\n- Jaeger o Zipkin para distributed tracing\n- Visualización de latencias entre servicios\n\n### 7. Seguridad\n\n**Autenticación:**\n- OAuth2 + OpenID Connect\n- JWT con refresh tokens (rotación cada 15 min)\n- MFA para admin\n\n**Autorización:**\n- RBAC (Role-Based Access Control)\n- Permisos granulares por servicio\n\n**Network:**\n- Service Mesh (Istio) para mTLS entre servicios\n- WAF (Web Application Firewall)\n- DDoS protection\n\n**Secrets:**\n- HashiCorp Vault o AWS Secrets Manager\n- Rotación automática de credenciales\n\n### 8. DevOps y CI/CD\n\n**Containerización:**\n- Docker para todos los servicios\n- Multi-stage builds para optimizar tamaño\n\n**Orquestación:**\n- Kubernetes (EKS, GKE, o AKS)\n- Helm charts para deployments\n- Horizontal Pod Autoscaler (HPA)\n\n**CI/CD:**\n- GitHub Actions o GitLab CI\n- Pipeline: Test → Build → Security Scan → Deploy\n- Blue-Green deployment o Canary releases\n- Rollback automático si error rate > 1%\n\n### 9. Escalabilidad\n\n**Horizontal Scaling:**\n- Auto-scaling basado en CPU (>70%) y requests/s\n- Min 2 pods, max 50 pods por servicio\n\n**Caching:**\n- CDN para assets estáticos\n- Redis para datos frecuentes (TTL 5-60 min)\n- HTTP caching headers\n\n**Database:**\n- Read replicas para PostgreSQL (1 master, 3 réplicas)\n- Sharding por región geográfica\n- Connection pooling (PgBouncer)\n\n### 10. Resiliencia\n\n**Patrones:**\n- Circuit Breaker: Evita cascading failures\n- Bulkhead: Aislamiento de recursos\n- Retry with exponential backoff\n- Timeout agresivos\n\n**Disaster Recovery:**\n- Multi-región active-active\n- RPO: 1 hora, RTO: 4 horas\n- Chaos engineering (Chaos Monkey)\n\n### 11. Flujo de Compra (Happy Path)\n\n```\n1. User busca producto → Product Service (Elasticsearch)\n2. User agrega al carrito → Order Service (Redis cache)\n3. User checkout → Order Service crea orden PENDING\n4. Order Service → Inventory Service: Reserva stock\n5. Order Service → Payment Service: Procesa pago\n6. Payment Service → Stripe API → Webhook confirmation\n7. Payment Service publica evento: PAYMENT_SUCCESS\n8. Order Service escucha evento → Actualiza orden a CONFIRMED\n9. Order Service → Shipping Service: Crea envío\n10. Notification Service escucha eventos → Envía email confirmación\n11. Analytics Service registra venta\n```\n\n### 12. Manejo de Fallos (Saga Pattern)\n\n**Si falla el pago:**\n```\n1. Payment Service retorna error\n2. Order Service ejecuta compensación:\n   - Libera stock en Inventory Service\n   - Actualiza orden a PAYMENT_FAILED\n   - Notification Service notifica al usuario\n```\n\n### 13. Costos Estimados (AWS, 10K pedidos/día)\n\n- Compute (EKS): ~$2000/mes\n- Databases (RDS, DocumentDB): ~$1500/mes\n- Kafka/Redis: ~$500/mes\n- Load Balancers: ~$300/mes\n- Monitoring: ~$200/mes\n- **Total**: ~$4500/mes\n\n### 14. Métricas Clave (SLIs/SLOs)\n\n- **Availability**: 99.9% uptime (8.76h downtime/año)\n- **Latency**: p95 < 500ms, p99 < 1s\n- **Error rate**: < 0.1%\n- **Order success rate**: > 99.5%\n\n### 15. Fases de Implementación\n\n**Fase 1 (MVP - 3 meses):**\n- User, Product, Order, Payment Services\n- PostgreSQL + Redis\n- Monolith deployment\n\n**Fase 2 (4-6 meses):**\n- Inventory, Shipping, Notification Services\n- Kafka para eventos\n- Kubernetes deployment\n\n**Fase 3 (7-12 meses):**\n- Analytics Service\n- Multi-región\n- Optimizaciones de performance\n\nEsta arquitectura permite escalar de 100 a 1M pedidos/día con ajustes en infraestructura, sin cambios en código."}
{"instruction": "Implementa un algoritmo de Machine Learning desde cero explicando cada paso matemático", "input": "", "output": "Voy a implementar **Regresión Logística** desde cero, incluyendo todas las derivadas matemáticas.\n\n## 1. Contexto del Problema\n\n**Objetivo**: Clasificación binaria (ej: spam vs no-spam)\n\n**Datos**: \n- X: matriz de features (n muestras × m features)\n- y: vector de etiquetas (0 o 1)\n\n## 2. Fundamentos Matemáticos\n\n### Función Sigmoide\n\nConvierte cualquier valor real en probabilidad [0, 1]:\n\n```\nσ(z) = 1 / (1 + e^(-z))\n```\n\n**Propiedades útiles:**\n- σ(0) = 0.5\n- σ(∞) = 1\n- σ(-∞) = 0\n- Derivada: σ'(z) = σ(z) × (1 - σ(z))\n\n### Hipótesis del Modelo\n\n```\nh(x) = σ(w^T × x + b)\n```\n\nDonde:\n- w: pesos (vector de tamaño m)\n- b: bias (escalar)\n- x: features de una muestra\n\n### Función de Pérdida (Cross-Entropy Loss)\n\nPara UNA muestra:\n\n```\nL(h(x), y) = -[y×log(h(x)) + (1-y)×log(1-h(x))]\n```\n\n**Intuición:**\n- Si y=1: queremos h(x)→1, entonces -log(h(x))→0\n- Si y=0: queremos h(x)→0, entonces -log(1-h(x))→0\n\nPara TODAS las muestras (promedio):\n\n```\nJ(w,b) = (1/n) × Σ L(h(x_i), y_i)\n```\n\n## 3. Derivación del Gradiente\n\n### Necesitamos calcular:\n- ∂J/∂w (gradiente respecto a pesos)\n- ∂J/∂b (gradiente respecto a bias)\n\n### Aplicando Regla de la Cadena:\n\n**Para un peso w_j:**\n\n```\n∂J/∂w_j = (1/n) × Σ ∂L/∂h × ∂h/∂z × ∂z/∂w_j\n```\n\nDesarrollando cada término:\n\n1. **∂L/∂h** (derivada de loss respecto a predicción):\n```\n∂L/∂h = -y/h + (1-y)/(1-h)\n       = (h - y) / (h×(1-h))\n```\n\n2. **∂h/∂z** (derivada de sigmoid):\n```\n∂h/∂z = h × (1 - h)\n```\n\n3. **∂z/∂w_j** (derivada de combinación lineal):\n```\n∂z/∂w_j = x_j\n```\n\n**Combinando (¡la magia!):**\n\n```\n∂J/∂w_j = (1/n) × Σ [(h - y) / (h×(1-h))] × [h×(1-h)] × x_j\n         = (1/n) × Σ (h - y) × x_j\n         = (1/n) × X^T × (h - y)\n```\n\n**Similarmente para bias:**\n```\n∂J/∂b = (1/n) × Σ (h - y)\n```\n\n## 4. Algoritmo de Descenso de Gradiente\n\n```\nRepetir hasta convergencia:\n    1. Calcular predicciones: h = σ(X×w + b)\n    2. Calcular gradientes:\n       dw = (1/n) × X^T × (h - y)\n       db = (1/n) × Σ(h - y)\n    3. Actualizar parámetros:\n       w = w - α × dw\n       b = b - α × db\n```\n\nDonde α es el learning rate (ej: 0.01)\n\n## 5. Implementación en Python\n\n```python\nimport numpy as np\n\nclass LogisticRegressionFromScratch:\n    def __init__(self, learning_rate=0.01, n_iterations=1000):\n        self.lr = learning_rate\n        self.n_iterations = n_iterations\n        self.weights = None\n        self.bias = None\n        self.losses = []  # Para visualizar convergencia\n    \n    def sigmoid(self, z):\n        \"\"\"Función sigmoide con protección contra overflow\"\"\"\n        # Clip para evitar overflow en exp\n        z = np.clip(z, -500, 500)\n        return 1 / (1 + np.exp(-z))\n    \n    def compute_loss(self, y_true, y_pred):\n        \"\"\"Cross-entropy loss\"\"\"\n        # Evitar log(0) con epsilon\n        epsilon = 1e-15\n        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n        \n        loss = -np.mean(\n            y_true * np.log(y_pred) + \n            (1 - y_true) * np.log(1 - y_pred)\n        )\n        return loss\n    \n    def fit(self, X, y):\n        \"\"\"\n        Entrena el modelo\n        \n        Args:\n            X: numpy array shape (n_samples, n_features)\n            y: numpy array shape (n_samples,)\n        \"\"\"\n        n_samples, n_features = X.shape\n        \n        # Inicialización de parámetros (Xavier initialization)\n        self.weights = np.random.randn(n_features) * np.sqrt(1/n_features)\n        self.bias = 0\n        \n        # Gradient descent\n        for i in range(self.n_iterations):\n            # 1. Forward pass (predicciones)\n            linear_output = np.dot(X, self.weights) + self.bias\n            y_pred = self.sigmoid(linear_output)\n            \n            # 2. Compute loss (para monitoreo)\n            loss = self.compute_loss(y, y_pred)\n            self.losses.append(loss)\n            \n            # 3. Backward pass (gradientes)\n            error = y_pred - y\n            dw = (1/n_samples) * np.dot(X.T, error)\n            db = (1/n_samples) * np.sum(error)\n            \n            # 4. Update parameters\n            self.weights -= self.lr * dw\n            self.bias -= self.lr * db\n            \n            # Logging cada 100 iteraciones\n            if i % 100 == 0:\n                print(f\"Iteration {i}: Loss = {loss:.4f}\")\n    \n    def predict_proba(self, X):\n        \"\"\"Retorna probabilidades [0, 1]\"\"\"\n        linear_output = np.dot(X, self.weights) + self.bias\n        return self.sigmoid(linear_output)\n    \n    def predict(self, X, threshold=0.5):\n        \"\"\"Retorna clases binarias {0, 1}\"\"\"\n        probabilities = self.predict_proba(X)\n        return (probabilities >= threshold).astype(int)\n    \n    def score(self, X, y):\n        \"\"\"Accuracy del modelo\"\"\"\n        y_pred = self.predict(X)\n        return np.mean(y_pred == y)\n\n# Ejemplo de uso\nif __name__ == \"__main__\":\n    # Generar datos sintéticos\n    from sklearn.datasets import make_classification\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    \n    # Crear dataset\n    X, y = make_classification(\n        n_samples=1000,\n        n_features=20,\n        n_informative=15,\n        n_redundant=5,\n        random_state=42\n    )\n    \n    # Split train/test\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    \n    # Normalización (importante para gradient descent)\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n    \n    # Entrenar modelo\n    model = LogisticRegressionFromScratch(\n        learning_rate=0.1,\n        n_iterations=1000\n    )\n    model.fit(X_train, y_train)\n    \n    # Evaluar\n    train_acc = model.score(X_train, y_train)\n    test_acc = model.score(X_test, y_test)\n    \n    print(f\"\\nTrain Accuracy: {train_acc:.4f}\")\n    print(f\"Test Accuracy: {test_acc:.4f}\")\n    \n    # Visualizar convergencia\n    import matplotlib.pyplot as plt\n    \n    plt.plot(model.losses)\n    plt.xlabel('Iteration')\n    plt.ylabel('Cross-Entropy Loss')\n    plt.title('Loss Convergence')\n    plt.grid(True)\n    plt.show()\n```\n\n## 6. Mejoras y Extensiones\n\n### A. Regularización L2 (Ridge)\n\nEvita overfitting penalizando pesos grandes:\n\n```\nJ_reg(w,b) = J(w,b) + (λ/(2n)) × ||w||²\n```\n\nGradientes modificados:\n```\ndw = (1/n) × X^T × (h - y) + (λ/n) × w\n```\n\n### B. Optimizadores Avanzados\n\n**Momentum:**\n```\nv_t = β × v_{t-1} + (1-β) × dw\nw = w - α × v_t\n```\n\n**Adam (Adaptive Moment Estimation):**\n```\nm_t = β1 × m_{t-1} + (1-β1) × dw\nv_t = β2 × v_{t-1} + (1-β2) × dw²\nm̂_t = m_t / (1 - β1^t)\nv̂_t = v_t / (1 - β2^t)\nw = w - α × m̂_t / (√v̂_t + ε)\n```\n\n### C. Mini-Batch Gradient Descent\n\nEn lugar de usar todas las muestras, usar batches pequeños (ej: 32):\n\n```python\nfor epoch in range(n_epochs):\n    for batch in get_batches(X, y, batch_size=32):\n        X_batch, y_batch = batch\n        # Calcular gradientes solo en batch\n        # Actualizar parámetros\n```\n\nVentajas:\n- Más rápido en datasets grandes\n- Permite paralelización\n- Añade ruido que ayuda a escapar mínimos locales\n\n## 7. Análisis de Complejidad\n\n**Temporal:**\n- Forward pass: O(n × m)\n- Backward pass: O(n × m)\n- Por iteración: O(n × m)\n- Total: O(iterations × n × m)\n\n**Espacial:**\n- Parámetros: O(m)\n- Gradientes: O(m)\n- Total: O(m)\n\n## 8. Comparación con Scikit-Learn\n\n```python\nfrom sklearn.linear_model import LogisticRegression as SklearnLR\n\nsklearn_model = SklearnLR(max_iter=1000)\nsklearn_model.fit(X_train, y_train)\nsk_acc = sklearn_model.score(X_test, y_test)\n\nprint(f\"Sklearn Accuracy: {sk_acc:.4f}\")\nprint(f\"Nuestra Accuracy: {test_acc:.4f}\")\n# Deberían ser muy similares (~0.01 diferencia)\n```\n\nEsta implementación captura todos los conceptos esenciales de machine learning: forward pass, loss computation, backpropagation, y gradient descent."}
{"instruction": "Explica cómo funciona un compilador y crea un mini-compilador para un lenguaje simple", "input": "", "output": "## Teoría: Fases de un Compilador\n\nUn compilador traduce código fuente a código máquina en varias fases:\n\n### 1. Análisis Léxico (Lexer/Tokenizer)\n- Entrada: Código fuente (string)\n- Salida: Secuencia de tokens\n- Función: Identifica palabras clave, identificadores, números, operadores\n\n**Ejemplo:**\n```\nCódigo: \"x = 5 + 3\"\nTokens: [ID(x), ASSIGN, NUMBER(5), PLUS, NUMBER(3)]\n```\n\n### 2. Análisis Sintáctico (Parser)\n- Entrada: Tokens\n- Salida: Árbol de Sintaxis Abstracta (AST)\n- Función: Verifica gramática, construye estructura jerárquica\n\n**Ejemplo:**\n```\nAST:\n    ASSIGN\n    /    \\\n   x      ADD\n         /   \\\n        5     3\n```\n\n### 3. Análisis Semántico\n- Verifica tipos de datos\n- Verifica declaraciones de variables\n- Inferencia de tipos\n\n### 4. Generación de Código Intermedio\n- Representación independiente de arquitectura\n- Ejemplo: Three-Address Code (TAC)\n\n### 5. Optimización\n- Elimina código muerto\n- Simplifica expresiones\n- Loop unrolling\n\n### 6. Generación de Código Máquina\n- Traduce a instrucciones específicas de CPU\n- Asignación de registros\n\n## Implementación: Mini-Compilador\n\nVoy a crear un compilador para un lenguaje simple que soporte:\n- Declaraciones de variables\n- Expresiones aritméticas\n- Compilación a Python bytecode\n\n### Gramática del Lenguaje\n\n```\nprogram    → statement*\nstatement  → varDecl | assignment | printStmt\nvarDecl    → \"var\" IDENTIFIER \"=\" expression\nassignment → IDENTIFIER \"=\" expression\nprintStmt  → \"print\" expression\nexpression → term ((\"plus\"/\"minus\") term)*\nterm       → factor ((\"times\"/\"div\") factor)*\nfactor     → NUMBER | IDENTIFIER | \"(\" expression \")\"\n```\n\n### Código Completo\n\n```python\nimport re\nfrom enum import Enum, auto\nfrom dataclasses import dataclass\nfrom typing import List, Union, Dict\n\n# ============== FASE 1: ANÁLISIS LÉXICO ==============\n\nclass TokenType(Enum):\n    # Literales\n    NUMBER = auto()\n    IDENTIFIER = auto()\n    \n    # Palabras clave\n    VAR = auto()\n    PRINT = auto()\n    \n    # Operadores\n    PLUS = auto()\n    MINUS = auto()\n    TIMES = auto()\n    DIV = auto()\n    ASSIGN = auto()\n    \n    # Delimitadores\n    LPAREN = auto()\n    RPAREN = auto()\n    \n    # Control\n    EOF = auto()\n\n@dataclass\nclass Token:\n    type: TokenType\n    value: Union[str, int, float]\n    line: int\n    column: int\n\nclass Lexer:\n    def __init__(self, source: str):\n        self.source = source\n        self.pos = 0\n        self.line = 1\n        self.column = 1\n        self.tokens = []\n        \n        self.keywords = {\n            'var': TokenType.VAR,\n            'print': TokenType.PRINT,\n            'plus': TokenType.PLUS,\n            'minus': TokenType.MINUS,\n            'times': TokenType.TIMES,\n            'div': TokenType.DIV,\n        }\n    \n    def current_char(self):\n        if self.pos >= len(self.source):\n            return None\n        return self.source[self.pos]\n    \n    def peek(self, offset=1):\n        pos = self.pos + offset\n        if pos >= len(self.source):\n            return None\n        return self.source[pos]\n    \n    def advance(self):\n        if self.current_char() == '\\n':\n            self.line += 1\n            self.column = 1\n        else:\n            self.column += 1\n        self.pos += 1\n    \n    def skip_whitespace(self):\n        while self.current_char() and self.current_char().isspace():\n            self.advance()\n    \n    def read_number(self):\n        num_str = ''\n        start_col = self.column\n        \n        while self.current_char() and (self.current_char().isdigit() or self.current_char() == '.'):\n            num_str += self.current_char()\n            self.advance()\n        \n        value = float(num_str) if '.' in num_str else int(num_str)\n        return Token(TokenType.NUMBER, value, self.line, start_col)\n    \n    def read_identifier(self):\n        id_str = ''\n        start_col = self.column\n        \n        while self.current_char() and (self.current_char().isalnum() or self.current_char() == '_'):\n            id_str += self.current_char()\n            self.advance()\n        \n        token_type = self.keywords.get(id_str, TokenType.IDENTIFIER)\n        return Token(token_type, id_str, self.line, start_col)\n    \n    def tokenize(self) -> List[Token]:\n        while self.current_char():\n            self.skip_whitespace()\n            \n            if not self.current_char():\n                break\n            \n            # Numbers\n            if self.current_char().isdigit():\n                self.tokens.append(self.read_number())\n            \n            # Identifiers and keywords\n            elif self.current_char().isalpha() or self.current_char() == '_':\n                self.tokens.append(self.read_identifier())\n            \n            # Operators and symbols\n            elif self.current_char() == '=':\n                self.tokens.append(Token(TokenType.ASSIGN, '=', self.line, self.column))\n                self.advance()\n            \n            elif self.current_char() == '(':\n                self.tokens.append(Token(TokenType.LPAREN, '(', self.line, self.column))\n                self.advance()\n            \n            elif self.current_char() == ')':\n                self.tokens.append(Token(TokenType.RPAREN, ')', self.line, self.column))\n                self.advance()\n            \n            else:\n                raise SyntaxError(f\"Unexpected character '{self.current_char()}' at line {self.line}, column {self.column}\")\n        \n        self.tokens.append(Token(TokenType.EOF, None, self.line, self.column))\n        return self.tokens\n\n# ============== FASE 2: AST (Abstract Syntax Tree) ==============\n\n@dataclass\nclass ASTNode:\n    pass\n\n@dataclass\nclass NumberNode(ASTNode):\n    value: Union[int, float]\n\n@dataclass\nclass VariableNode(ASTNode):\n    name: str\n\n@dataclass\nclass BinaryOpNode(ASTNode):\n    left: ASTNode\n    operator: TokenType\n    right: ASTNode\n\n@dataclass\nclass VarDeclNode(ASTNode):\n    name: str\n    value: ASTNode\n\n@dataclass\nclass AssignmentNode(ASTNode):\n    name: str\n    value: ASTNode\n\n@dataclass\nclass PrintNode(ASTNode):\n    expression: ASTNode\n\n@dataclass\nclass ProgramNode(ASTNode):\n    statements: List[ASTNode]\n\n# ============== FASE 3: PARSER ==============\n\nclass Parser:\n    def __init__(self, tokens: List[Token]):\n        self.tokens = tokens\n        self.pos = 0\n    \n    def current_token(self):\n        if self.pos < len(self.tokens):\n            return self.tokens[self.pos]\n        return self.tokens[-1]  # EOF\n    \n    def eat(self, token_type: TokenType):\n        if self.current_token().type == token_type:\n            token = self.current_token()\n            self.pos += 1\n            return token\n        else:\n            raise SyntaxError(f\"Expected {token_type}, got {self.current_token().type}\")\n    \n    def parse(self) -> ProgramNode:\n        statements = []\n        \n        while self.current_token().type != TokenType.EOF:\n            statements.append(self.statement())\n        \n        return ProgramNode(statements)\n    \n    def statement(self):\n        if self.current_token().type == TokenType.VAR:\n            return self.var_decl()\n        elif self.current_token().type == TokenType.PRINT:\n            return self.print_stmt()\n        elif self.current_token().type == TokenType.IDENTIFIER:\n            return self.assignment()\n        else:\n            raise SyntaxError(f\"Unexpected token: {self.current_token()}\")\n    \n    def var_decl(self):\n        self.eat(TokenType.VAR)\n        name = self.eat(TokenType.IDENTIFIER).value\n        self.eat(TokenType.ASSIGN)\n        value = self.expression()\n        return VarDeclNode(name, value)\n    \n    def assignment(self):\n        name = self.eat(TokenType.IDENTIFIER).value\n        self.eat(TokenType.ASSIGN)\n        value = self.expression()\n        return AssignmentNode(name, value)\n    \n    def print_stmt(self):\n        self.eat(TokenType.PRINT)\n        expr = self.expression()\n        return PrintNode(expr)\n    \n    def expression(self):\n        node = self.term()\n        \n        while self.current_token().type in (TokenType.PLUS, TokenType.MINUS):\n            op = self.current_token().type\n            self.pos += 1\n            node = BinaryOpNode(node, op, self.term())\n        \n        return node\n    \n    def term(self):\n        node = self.factor()\n        \n        while self.current_token().type in (TokenType.TIMES, TokenType.DIV):\n            op = self.current_token().type\n            self.pos += 1\n            node = BinaryOpNode(node, op, self.factor())\n        \n        return node\n    \n    def factor(self):\n        token = self.current_token()\n        \n        if token.type == TokenType.NUMBER:\n            self.pos += 1\n            return NumberNode(token.value)\n        \n        elif token.type == TokenType.IDENTIFIER:\n            self.pos += 1\n            return VariableNode(token.value)\n        \n        elif token.type == TokenType.LPAREN:\n            self.eat(TokenType.LPAREN)\n            node = self.expression()\n            self.eat(TokenType.RPAREN)\n            return node\n        \n        else:\n            raise SyntaxError(f\"Unexpected token in factor: {token}\")\n\n# ============== FASE 4: GENERACIÓN DE CÓDIGO ==============\n\nclass CodeGenerator:\n    def __init__(self):\n        self.code = []\n        self.variables = set()\n    \n    def generate(self, ast: ProgramNode) -> str:\n        for statement in ast.statements:\n            self.visit(statement)\n        \n        return '\\n'.join(self.code)\n    \n    def visit(self, node: ASTNode):\n        method_name = f'visit_{type(node).__name__}'\n        method = getattr(self, method_name, self.generic_visit)\n        return method(node)\n    \n    def generic_visit(self, node):\n        raise Exception(f'No visit method for {type(node).__name__}')\n    \n    def visit_VarDeclNode(self, node: VarDeclNode):\n        self.variables.add(node.name)\n        expr_code = self.visit(node.value)\n        self.code.append(f\"{node.name} = {expr_code}\")\n    \n    def visit_AssignmentNode(self, node: AssignmentNode):\n        if node.name not in self.variables:\n            raise NameError(f\"Variable '{node.name}' not declared\")\n        expr_code = self.visit(node.value)\n        self.code.append(f\"{node.name} = {expr_code}\")\n    \n    def visit_PrintNode(self, node: PrintNode):\n        expr_code = self.visit(node.expression)\n        self.code.append(f\"print({expr_code})\")\n    \n    def visit_BinaryOpNode(self, node: BinaryOpNode):\n        left = self.visit(node.left)\n        right = self.visit(node.right)\n        \n        op_map = {\n            TokenType.PLUS: '+',\n            TokenType.MINUS: '-',\n            TokenType.TIMES: '*',\n            TokenType.DIV: '/'\n        }\n        \n        op = op_map[node.operator]\n        return f\"({left} {op} {right})\"\n    \n    def visit_NumberNode(self, node: NumberNode):\n        return str(node.value)\n    \n    def visit_VariableNode(self, node: VariableNode):\n        if node.name not in self.variables:\n            raise NameError(f\"Variable '{node.name}' not declared\")\n        return node.name\n\n# ============== COMPILADOR COMPLETO ==============\n\nclass Compiler:\n    def compile(self, source_code: str) -> str:\n        # Fase 1: Lexer\n        lexer = Lexer(source_code)\n        tokens = lexer.tokenize()\n        print(\"\\n=== TOKENS ===\")\n        for token in tokens:\n            print(token)\n        \n        # Fase 2: Parser\n        parser = Parser(tokens)\n        ast = parser.parse()\n        print(\"\\n=== AST ===\")\n        print(ast)\n        \n        # Fase 3: Code Generation\n        codegen = CodeGenerator()\n        python_code = codegen.generate(ast)\n        print(\"\\n=== GENERATED CODE ===\")\n        print(python_code)\n        \n        return python_code\n    \n    def compile_and_run(self, source_code: str):\n        python_code = self.compile(source_code)\n        print(\"\\n=== EXECUTION ===\")\n        exec(python_code)\n\n# ============== EJEMPLO DE USO ==============\n\nif __name__ == \"__main__\":\n    source = \"\"\"\n    var x = 10\n    var y = 5 plus 3\n    var z = x times y\n    print z\n    z = z plus 10\n    print z\n    \"\"\"\n    \n    compiler = Compiler()\n    compiler.compile_and_run(source)\n```\n\n## Salida del Programa\n\n```\n=== TOKENS ===\nToken(type=<TokenType.VAR: 3>, value='var', line=2, column=5)\nToken(type=<TokenType.IDENTIFIER: 2>, value='x', line=2, column=9)\nToken(type=<TokenType.ASSIGN: 11>, value='=', line=2, column=11)\nToken(type=<TokenType.NUMBER: 1>, value=10, line=2, column=13)\n...\n\n=== AST ===\nProgramNode(statements=[...])\n\n=== GENERATED CODE ===\nx = 10\ny = (5 + 3)\nz = (x * y)\nprint(z)\nz = (z + 10)\nprint(z)\n\n=== EXECUTION ===\n80\n90\n```\n\n## Extensiones Posibles\n\n1. **Tipos de datos**: Agregar strings, booleans\n2. **Control de flujo**: if, while, for\n3. **Funciones**: Definición y llamadas\n4. **Optimizador**: Constant folding (5 + 3 → 8)\n5. **Target diferente**: Generar assembly o LLVM IR\n6. **Type checking**: Análisis semántico avanzado\n\nEste compilador demuestra todos los conceptos fundamentales de diseño de compiladores."}
