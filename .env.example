# Model Configuration
MODEL_NAME=TinyLlama/TinyLlama-1.1B-Chat-v1.0
MODEL_PATH=./data/models
DEVICE=auto  # Options: auto, cuda, mps, cpu

# Training Configuration
BATCH_SIZE=4
LEARNING_RATE=2e-5
MAX_LENGTH=2048
NUM_EPOCHS=3
GRADIENT_ACCUMULATION_STEPS=4
WARMUP_STEPS=100

# Generation Parameters
TEMPERATURE=0.7
TOP_P=0.9
TOP_K=50
MAX_NEW_TOKENS=512

# Memory Configuration
MEMORY_DB_PATH=./data/memory/chroma_db
EPISODIC_DB_PATH=./data/memory/episodic.db
SHORT_TERM_MEMORY_SIZE=4096
MAX_LONG_TERM_MEMORIES=10000

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=1
CORS_ORIGINS=["http://localhost:3000", "http://localhost:8000"]

# Logging
LOG_LEVEL=INFO  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_FILE=./data/logs/my-llm.log

# Optional: Weights & Biases
WANDB_PROJECT=my-llm
WANDB_ENTITY=your-username
WANDB_API_KEY=your-api-key-here

# Optional: HuggingFace Token (for private models)
HF_TOKEN=your-huggingface-token-here

# Performance
USE_FLASH_ATTENTION=false
USE_QUANTIZATION=true  # 8-bit quantization
QUANTIZATION_BITS=8  # Options: 4, 8
