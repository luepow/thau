# ğŸ¤– THAU - Manual Completo del Sistema

**Trainable Helpful AI Unit** - Sistema de Entrenamiento AutÃ³nomo para LLMs

---

## ğŸ“‘ Tabla de Contenidos

1. [IntroducciÃ³n](#introducciÃ³n)
2. [CaracterÃ­sticas Principales](#caracterÃ­sticas-principales)
3. [Arquitectura del Sistema](#arquitectura-del-sistema)
4. [InstalaciÃ³n](#instalaciÃ³n)
5. [Inicio RÃ¡pido](#inicio-rÃ¡pido)
6. [Desarrollo Cognitivo](#desarrollo-cognitivo)
7. [Auto-Aprendizaje](#auto-aprendizaje)
8. [Memoria Vectorizada](#memoria-vectorizada)
9. [Aprendizaje MultilingÃ¼e](#aprendizaje-multilingÃ¼e)
10. [Protocolo MCP](#protocolo-mcp)
11. [API Endpoints](#api-endpoints)
12. [Casos de Uso](#casos-de-uso)
13. [ConfiguraciÃ³n Avanzada](#configuraciÃ³n-avanzada)
14. [Troubleshooting](#troubleshooting)
15. [Roadmap](#roadmap)

---

## IntroducciÃ³n

THAU es un sistema revolucionario que permite entrenar modelos LLM de forma autÃ³noma, sin consumir tus tokens de API. El modelo aprende progresivamente como un humano, desde edad 0 (reciÃ©n nacido) hasta 15+ aÃ±os (adulto experto).

### Â¿QuÃ© hace diferente a THAU?

| CaracterÃ­stica | THAU | Modelos Tradicionales |
|---|---|---|
| **Entrenamiento** | AutÃ³nomo, sin tokens | Requiere tokens/GPU costosos |
| **Desarrollo** | Progresivo por edades | MonolÃ­tico |
| **Memoria** | Vectorizada eficiente | Limitada al contexto |
| **Idiomas** | MultilingÃ¼e con fonÃ©tica | Depende del dataset |
| **Auto-mejora** | Detecta y cubre brechas | EstÃ¡tico |
| **MCP** | Soporta herramientas | Limitado |

---

## CaracterÃ­sticas Principales

### 1. ğŸ§  Desarrollo Cognitivo por Edades

THAU aprende progresivamente en 7 etapas:

- **Edad 0** (ReciÃ©n Nacido): Palabras clave, respuestas simples
- **Edad 1-2** (Infante): Frases cortas, conceptos bÃ¡sicos
- **Edad 3-5** (NiÃ±o PequeÃ±o): Explicaciones simples, causa-efecto
- **Edad 6-10** (NiÃ±o): MatemÃ¡ticas bÃ¡sicas, lÃ³gica
- **Edad 11-12** (Pre-adolescente): Pensamiento abstracto
- **Edad 13-15** (Adolescente): Razonamiento complejo
- **Edad 15+** (Adulto): Expertise tÃ©cnico, tool calling

### 2. ğŸ”„ Auto-GeneraciÃ³n de Datasets

THAU puede:
- Detectar brechas de conocimiento automÃ¡ticamente
- Generar sus propios datasets de entrenamiento
- Cubrir Ã¡reas donde tiene respuestas inciertas
- Crear ejemplos apropiados para su edad cognitiva

### 3. ğŸ’¾ Memoria Vectorizada Eficiente

- **FAISS** o numpy para bÃºsqueda ultrarrÃ¡pida
- **Sentence Transformers** para embeddings de calidad
- CompresiÃ³n y gestiÃ³n inteligente
- RecuperaciÃ³n semÃ¡ntica de interacciones previas

### 4. ğŸŒ Aprendizaje MultilingÃ¼e

THAU puede aprender mÃºltiples idiomas:
- **Vocabulario**: Palabras con definiciones, ejemplos
- **FonÃ©tica**: PronunciaciÃ³n IPA, divisiÃ³n silÃ¡bica
- **GramÃ¡tica**: Reglas lingÃ¼Ã­sticas
- **TraducciÃ³n**: Mapeo entre idiomas

Idiomas soportados: EspaÃ±ol, InglÃ©s, FrancÃ©s, AlemÃ¡n, Italiano, PortuguÃ©s

### 5. ğŸ”— Protocolo MCP

ImplementaciÃ³n completa del Model Context Protocol de Anthropic:
- **Herramientas**: web_search, execute_python, recall_memory, learn_word, generate_dataset
- **Recursos**: Acceso a conocimiento estructurado
- **Interoperabilidad**: Compatible con Claude y otros sistemas MCP

---

## Arquitectura del Sistema

```
my-llm/
â”œâ”€â”€ thau_trainer/                 # Core del sistema
â”‚   â”œâ”€â”€ cognitive_development.py  # GestiÃ³n de edades cognitivas
â”‚   â”œâ”€â”€ self_learning.py          # Auto-generaciÃ³n de datasets
â”‚   â”œâ”€â”€ vector_memory.py          # Memoria vectorizada
â”‚   â”œâ”€â”€ language_learning.py      # Sistema multilingÃ¼e
â”‚   â”œâ”€â”€ mcp_server.py             # Servidor MCP
â”‚   â””â”€â”€ integrated_trainer.py    # Coordinador principal
â”‚
â”œâ”€â”€ api/
â”‚   â””â”€â”€ thau_api_integrated.py   # API FastAPI completa
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ datasets/                # Datasets por edad
â”‚   â”‚   â”œâ”€â”€ age_0_newborn.jsonl
â”‚   â”‚   â”œâ”€â”€ age_1_infant.jsonl
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â””â”€â”€ auto_generated/      # Datasets auto-generados
â”‚   â”œâ”€â”€ memory/                  # Ãndices vectoriales
â”‚   â”œâ”€â”€ language/                # Vocabularios y fonÃ©tica
â”‚   â””â”€â”€ logs/                    # Logs y progreso
â”‚
â””â”€â”€ start_thau.sh                # Script de inicio
```

---

## InstalaciÃ³n

### Requisitos Previos

1. **Python 3.10+**
   ```bash
   python3 --version
   ```

2. **Ollama** ([instalar](https://ollama.ai))
   ```bash
   curl https://ollama.ai/install.sh | sh
   ```

3. **Git**
   ```bash
   git --version
   ```

### Pasos de InstalaciÃ³n

```bash
# 1. Clonar repositorio
cd /path/to/your/projects
git clone <repo-url> thau_1_0
cd thau_1_0/my-llm

# 2. Crear entorno virtual
python3 -m venv venv
source venv/bin/activate

# 3. Instalar dependencias
pip install -r requirements.txt

# 4. Iniciar Ollama (en terminal separada)
ollama serve

# 5. Descargar modelo base
ollama pull qwen2.5-coder:1.5b-base

# 6. Verificar instalaciÃ³n
python thau_trainer/integrated_trainer.py
```

---

## Inicio RÃ¡pido

### OpciÃ³n A: Usando el script

```bash
# Iniciar API server
./start_thau.sh

# O probar el sistema
./start_thau.sh test

# O probar MCP
./start_thau.sh mcp
```

### OpciÃ³n B: Manual

```bash
# Activar entorno
source venv/bin/activate

# Iniciar API
python api/thau_api_integrated.py
```

### Primer Uso

```bash
# Verificar que estÃ¡ corriendo
curl http://localhost:8000/health

# Ver estado
curl http://localhost:8000/status

# Procesar primera interacciÃ³n
curl -X POST http://localhost:8000/interact \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Â¿QuÃ© es Python?",
    "answer": "Python es un lenguaje de programaciÃ³n de alto nivel",
    "confidence": 0.9
  }'

# Ver documentaciÃ³n interactiva
# http://localhost:8000/docs
```

---

## Desarrollo Cognitivo

### Conceptos Clave

Cada edad tiene:
- **Capacidades**: QuÃ© puede hacer
- **Dominios de aprendizaje**: QuÃ© puede aprender
- **Complejidad de razonamiento**: Nivel 1-10
- **Longitud de contexto**: Tokens que puede manejar
- **Criterios de avance**: Requisitos para siguiente edad

### Ver Estado Actual

```bash
# API
curl http://localhost:8000/cognitive/status

# CLI
python -c "
from thau_trainer.cognitive_development import CognitiveDevelopmentManager
mgr = CognitiveDevelopmentManager()
import json
print(json.dumps(mgr.get_status(), indent=2))
"
```

### Forzar Avance de Edad

```bash
# Solo avanza si cumple criterios
curl -X POST http://localhost:8000/cognitive/advance
```

### Progreso de Ejemplo

```
Edad 0 â†’ 100 ejemplos, 70% accuracy â†’ Edad 1
Edad 1 â†’ 200 ejemplos, 75% accuracy â†’ Edad 3
Edad 3 â†’ 500 ejemplos, 80% accuracy â†’ Edad 6
...
```

---

## Auto-Aprendizaje

### DetecciÃ³n de Brechas

THAU detecta automÃ¡ticamente cuando:
- Respuestas muy cortas (< 20 caracteres)
- Marcadores de incertidumbre ("no estoy seguro", "no sÃ©")
- Confianza baja (< 0.6)

### GeneraciÃ³n AutomÃ¡tica

```bash
# Generar datasets para brechas
curl -X POST "http://localhost:8000/auto-improve?min_gaps=3"
```

### Proceso Completo

1. **Usuario interactÃºa** â†’ THAU responde
2. **Sistema detecta brecha** â†’ Registra tÃ³pico
3. **Auto-mejora ejecuta** â†’ Genera 5-10 ejemplos
4. **Dataset creado** â†’ Se importa a cola
5. **Entrenamiento** â†’ THAU aprende

### Ver Brechas Detectadas

```bash
# Logs
cat data/logs/knowledge_gaps.jsonl | jq

# Stats
curl http://localhost:8000/stats/self-learning
```

---

## Memoria Vectorizada

### CaracterÃ­sticas

- **BÃºsqueda semÃ¡ntica**: No necesita palabras exactas
- **Escalable**: Hasta 10,000+ vectores
- **RÃ¡pida**: Milisegundos por bÃºsqueda
- **Eficiente**: Auto-limpieza

### BÃºsqueda

```bash
# API
curl -X POST http://localhost:8000/memory/recall \
  -H "Content-Type: application/json" \
  -d '{
    "query": "programaciÃ³n orientada a objetos",
    "k": 3
  }'

# Python
from thau_trainer.vector_memory import EfficientVectorMemory

memory = EfficientVectorMemory()
results = memory.search("machine learning", k=5)

for result in results:
    print(f"Score: {result['score']:.3f}")
    print(f"Text: {result['text']}")
```

### EstadÃ­sticas

```bash
curl http://localhost:8000/stats/memory
```

---

## Aprendizaje MultilingÃ¼e

### AÃ±adir Idioma

```bash
curl -X POST "http://localhost:8000/language/add?language_code=en"
```

### Aprender Palabra

```bash
curl -X POST http://localhost:8000/language/learn-word \
  -H "Content-Type: application/json" \
  -d '{
    "word": "algorithm",
    "language": "en",
    "definition": "A step-by-step procedure to solve a problem",
    "pos": "noun",
    "examples": ["This sorting algorithm is efficient"]
  }'
```

### FonÃ©tica AutomÃ¡tica

Para espaÃ±ol, THAU genera automÃ¡ticamente:
- **IPA**: NotaciÃ³n fonÃ©tica internacional
- **SÃ­labas**: DivisiÃ³n silÃ¡bica
- **AcentuaciÃ³n**: SÃ­laba tÃ³nica

Ejemplo:
```
computadora â†’ /komputadora/ â†’ com-pu-ta-do-ra (tÃ³nica: "do")
```

### Ver Progreso

```bash
curl http://localhost:8000/language/progress/es
```

---

## Protocolo MCP

### Herramientas Disponibles

```bash
# Listar
curl http://localhost:8000/mcp/tools
```

**Herramientas:**

1. **web_search**: BÃºsqueda web
2. **execute_python**: Ejecutar cÃ³digo Python
3. **recall_memory**: Buscar en memoria
4. **learn_word**: Aprender vocabulario
5. **generate_dataset**: Crear datasets

### Llamar Herramienta

```bash
curl -X POST http://localhost:8000/mcp/call \
  -H "Content-Type: application/json" \
  -d '{
    "tool_name": "execute_python",
    "arguments": {
      "code": "print(\"Hello from THAU!\")\nresult = 2 ** 10\nprint(f\"2^10 = {result}\")"
    }
  }'
```

### IntegraciÃ³n con Claude

THAU puede comunicarse con Claude Desktop u otros clientes MCP:

1. Configurar cliente MCP para apuntar a `http://localhost:8000/mcp`
2. Claude puede llamar herramientas de THAU
3. Bidireccional: THAU puede usar herramientas externas

---

## API Endpoints

### Core

- `GET /` - Info del servidor
- `GET /status` - Estado completo
- `GET /health` - Health check

### Interacciones

- `POST /interact` - Procesar interacciÃ³n
- `POST /memory/recall` - Buscar en memoria
- `POST /train` - Entrenar ahora
- `POST /auto-improve` - Auto-mejorar

### Desarrollo Cognitivo

- `GET /cognitive/status` - Estado cognitivo
- `POST /cognitive/advance` - Avanzar edad

### Idiomas

- `POST /language/add` - AÃ±adir idioma
- `POST /language/learn-word` - Aprender palabra
- `GET /language/progress/{lang}` - Ver progreso

### MCP

- `GET /mcp/tools` - Listar herramientas
- `POST /mcp/call` - Ejecutar herramienta
- `GET /mcp/resources` - Listar recursos

### EstadÃ­sticas

- `GET /stats/memory` - Stats de memoria
- `GET /stats/self-learning` - Stats de auto-aprendizaje
- `GET /stats/datasets` - Datasets generados

---

## Casos de Uso

### Caso 1: Asistente Personal que Aprende

```python
import requests

BASE_URL = "http://localhost:8000"

# 1. EnseÃ±arle sobre tu proyecto
requests.post(f"{BASE_URL}/interact", json={
    "question": "Â¿CÃ³mo funciona mi mÃ³dulo de autenticaciÃ³n?",
    "answer": "Tu mÃ³dulo usa OAuth 2.0 con refresh tokens...",
    "confidence": 0.95
})

# 2. THAU recuerda
response = requests.post(f"{BASE_URL}/memory/recall", json={
    "query": "autenticaciÃ³n",
    "k": 3
})

print(response.json())
```

### Caso 2: Aprendizaje MultilingÃ¼e

```python
# AÃ±adir francÃ©s
requests.post(f"{BASE_URL}/language/add?language_code=fr")

# Aprender vocabulario tÃ©cnico
words = [
    ("ordinateur", "fr", "computadora/computer"),
    ("programmation", "fr", "programaciÃ³n/programming"),
    ("algorithme", "fr", "algoritmo/algorithm")
]

for word, lang, definition in words:
    requests.post(f"{BASE_URL}/language/learn-word", json={
        "word": word,
        "language": lang,
        "definition": definition
    })

# Ver progreso
progress = requests.get(f"{BASE_URL}/language/progress/fr")
print(f"Palabras aprendidas: {progress.json()['vocabulary_stats']['total_words']}")
```

### Caso 3: Entrenamiento Continuo

```python
# Loop de entrenamiento automÃ¡tico
import time

while True:
    # Usuario interactÃºa
    interaction = get_user_interaction()  # Tu lÃ³gica

    # THAU aprende
    requests.post(f"{BASE_URL}/interact", json=interaction)

    # Cada 100 interacciones, auto-mejorar
    if interaction_count % 100 == 0:
        requests.post(f"{BASE_URL}/auto-improve?min_gaps=5")

    # Cada 500, entrenar
    if interaction_count % 500 == 0:
        requests.post(f"{BASE_URL}/train")

    time.sleep(1)
```

---

## ConfiguraciÃ³n Avanzada

### Ajustar Criterios de Edad

Edita `thau_trainer/cognitive_development.py`:

```python
# Ejemplo: Facilitar avance de edad 3
advancement_criteria={
    "min_examples": 300,  # Reducir de 500
    "min_accuracy": 0.75,  # Reducir de 0.80
    "can_explain_simple_concepts": True
}
```

### Cambiar Intervalo de Auto-Mejora

En `start_thau.sh` o al inicializar:

```python
thau_trainer.start_auto_improvement_loop(interval_hours=1)  # Cada 1 hora
```

### Memoria Vectorizada: FAISS vs Numpy

Por defecto, THAU usa FAISS si estÃ¡ disponible, sino usa numpy.

Para forzar numpy:

```python
from thau_trainer.vector_memory import EfficientVectorMemory

memory = EfficientVectorMemory(index_type="flat")
```

Para IVF (rÃ¡pido):

```python
memory = EfficientVectorMemory(index_type="IVF")
```

Para HNSW (muy rÃ¡pido):

```python
memory = EfficientVectorMemory(index_type="HNSW")
```

---

## Troubleshooting

### "THAU no avanza de edad"

**SoluciÃ³n:**
```bash
# Ver progreso
curl http://localhost:8000/cognitive/status

# Verificar criterios
# - Â¿Tiene suficientes ejemplos?
# - Â¿El accuracy es suficiente?

# AÃ±adir mÃ¡s datos apropiados para la edad
```

### "Auto-mejora no genera datasets"

**Causas:**
- Menos de `min_gaps` brechas detectadas
- Ollama no estÃ¡ corriendo

**SoluciÃ³n:**
```bash
# Verificar Ollama
curl http://localhost:11434/api/version

# Ver brechas detectadas
curl http://localhost:8000/stats/self-learning

# Forzar con min_gaps=1
curl -X POST "http://localhost:8000/auto-improve?min_gaps=1"
```

### "Memoria vectorizada muy lenta"

**SoluciÃ³n:**
```bash
# Instalar FAISS
pip install faiss-cpu

# O limpiar memoria
curl http://localhost:8000/stats/memory
# Si > 10,000 vectores, se auto-limpia
```

### "Error al importar sentence-transformers"

**SoluciÃ³n:**
```bash
pip install sentence-transformers
```

THAU funcionarÃ¡ sin ella (usa embeddings simples), pero es mejor tenerla.

---

## Roadmap

### VersiÃ³n 1.1 (Q1 2025)

- [ ] IntegraciÃ³n con APIs de bÃºsqueda reales
- [ ] Soporte para mÃ¡s formatos de datasets (CSV, JSON)
- [ ] Dashboard web para monitoreo
- [ ] Exportar modelo entrenado a GGUF

### VersiÃ³n 1.2 (Q2 2025)

- [ ] Multimodalidad (imÃ¡genes, audio)
- [ ] IntegraciÃ³n con mÃ¡s idiomas (Ã¡rabe, chino, japonÃ©s)
- [ ] Sistema de recompensas (RLHF)
- [ ] Distributed training

### VersiÃ³n 2.0 (Q3 2025)

- [ ] Auto-arquitectura (THAU elige su estructura)
- [ ] Meta-aprendizaje (aprende a aprender mejor)
- [ ] Federated learning
- [ ] IntegraciÃ³n con Claude 4 API

---

## Contribuir

THAU es de cÃ³digo abierto. Pull requests son bienvenidos!

1. Fork el proyecto
2. Crea tu feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la branch (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

---

## Licencia

MIT License - Ve `LICENSE` para detalles

---

## Agradecimientos

- **Anthropic** - Por el protocolo MCP
- **Ollama** - Por hacer LLMs locales accesibles
- **Meta** - Por FAISS
- **HuggingFace** - Por transformers y datasets

---

## Soporte

- **DocumentaciÃ³n**: `THAU_FINAL_GUIDE.md`
- **GitHub Issues**: [Reportar bug](https://github.com/your-repo/issues)
- **Email**: support@thau-ai.com

---

**Â¡THAU crece mientras tÃº desarrollas!** ğŸŒ±ğŸ¤–
